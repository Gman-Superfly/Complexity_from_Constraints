# Complexity from Constraints: FEP for Coordination in Learning Systems

*Oscar Goldman (@Gman-Superfly) â€“ November 11 2025*

This is not a formal paper.  
It is simply a short note that ties together the loose threads running through all my public repositories.  

Itâ€™s a short technical note explaining the practical framework used across my public repos. The goal is concrete: document a small set of recurring energy-based patterns we use to coordinate small modules, and how this repository implements them in a reusable way.

I did not set out to find a thread between projects; this is work that sparks my interests and converged on similar mechanics over time.  

I am obsessively trying to make hard problems easier for myself to understand,  inverse reconstruction, training loops, agents, music, manifolds, hallucinations... sometimes have solutions that feel inevitable as most physical systems do.
A simple scalar objective plus the ability for the future to non-locally correct the past turns out to be enough. (ahem... enough for my simple brain to work on without exploding)

We focus on making hard problems more manageable with a simple recipe: each module exposes an order parameter and a local energy; sparse couplings allow â€œfuture-likeâ€ context to redeem past decisions; a coordinator relaxes the system by descending a total energy; gate decisions are made only when they reduce total energy and justify their cost.

### Technical description (what this framework is)

- Energy-based coordination: small modules expose order parameters `Î·` and local energies `F_local(Î·; c)`; sparse couplings add non-local structure.
- Relaxation loop: descend total energy `ğ“• = Î£ F_local + Î£ F_coupling` with guardrails (line search, invariants) and optional coordinate descent warm-start.
- Gating: rare-but-impactful expansions, opened only when expected free-energy drop exceeds a calibrated cost.
- Observability: traces for `Î”F`/Î·, simple KPIs, and hooks for weight adaptation/balancing.
- Scope and tone: this consolidates recurring patterns used across Gmanâ€‘Superfly repos into a readable, small framework. It builds on prior work; it does not claim novelty beyond careful composition and implementation hygiene.

### Core recurring equations

In our repos, many subsystems use specializations of the following:

1. **Local energy (Landau-Ginzburg style)**  
   $$F_i(\eta_i) = a_i \eta_i^2 + b_i \eta_i^4 - h_i \eta_i$$

2. **Non-local redemption coupling (future corrects past)**  
   $$C_{j \to i}(\eta_j, \eta_i) = \lambda_{ji} \left[ d(\hat{y}_i, f(\eta_j)) - m \right]^+$$

3. **Total free energy**  
   $$\mathcal{F} = \sum_i F_i(\eta_i) + \sum_{j,i} C_{j \to i} + \gamma \cdot N_{\text{modules}}$$

4. **Energy-gated expansion**  
   $$\Delta \mathcal{F}_{\text{new}} < -\tau \quad \Rightarrow \quad \text{add module (pay complexity cost } \gamma\text{)}$$

5. **Relaxation dynamics**  
   $$\dot{\eta}_i = -\frac{\partial \mathcal{F}}{\partial \eta_i}$$

These forms recur throughout the repos; this framework implements and composes them in a small, typed codebase.

### Where the Equations Appear

| Repository                               | What Î· represents                     | Redemption looks like                          | Gating decides                          | Result                                      |
|------------------------------------------|--------------------------------------|------------------------------------------------|-----------------------------------------|---------------------------------------------|
| Complexity_from_Constraints               | generic order parameter               | explicit futureâ†’past couplings                  | when to add new module                 | the primitive itself                         |
| Inverse_ND_Reconstruction                 | loop / trajectory parameters          | refinement stage corrects hallucinated loops     | which diffusion candidates survive       | explainable closed-loop reconstruction       |
| Normalized_Dynamic_OPT                   | cluster centers / kernel params       | later points reassign provisional points           | when to split clusters or add dims       | 83 % dataset compression, full biology kept  |
| Hallucinations_Noisy_Channels             | latent state along sequence           | later tokens want to correct earlier (blocked)  | (none â€“ shows what happens when missing) | information-theoretic theory of hallucinations|
| HMPO / AGM_Training                      | policy / value offsets               | harmonic mean as risk-averse correction         | adaptive temperature / trust region      | safer, more stable RL                       |
| Chromatic_Descent                        | network parameters in function space   | repulsion pushes solutions apart on palette     | (implicit in ensemble selection)         | low-D manifold of good minima                |
| Claudio (music agents)                   | rhythm/harmonic tension per agent    | Conductor/WildCard override earlier agents       | when to activate chaos/fractal modes     | coherent multi-agent music without central control |
| Spaced_Repetition_Learning               | replay priority of trajectories        | hard/diverse samples force correction           | when to keep or evict from buffer       | inference-time self-improvement              |
| Without_Noise_There_Is_Nothing           | stochastic resonance schedule         | noise lets system escape uncorrected minima      | cyclical temperature gating of noise strength    | noise is dual of redemption                  |
| dataset_quality_score                     | sublinear reward for data curation    | edge-case emphasis corrects sampling bias        | when to accept new datapoint            | RL-driven dataset improvement                |

Even the seemingly pure-math ones (Odd_VS_Even_Zeta_Substructure, sublinear_monotonicity_score) are consequences: certain structures lower the free energy under compression bases.

### Terminology: why â€œredemptionâ€

- Plain-language intent
  - We use â€œredemptionâ€ to describe when later context improves earlier, provisional decisions. It is not moral or mystical; itâ€™s a concise way to say â€œfutureâ€‘like evidence can lower the energy assigned to past choices.â€ In practice, this helps crossâ€‘disciplinary teams grasp the mechanism quickly.

- Technical mapping (what â€œredemptionâ€ means in math/code)
  - Nonâ€‘local correction is instantiated by coupling terms and a gate that only admits changes when they reduce total energy:
    - Hingeâ€‘style coupling (future corrects past): \(C_{j\to i} = \lambda_{ji}\,[\,d(\hat y_i, f(\eta_j)) - m\,]^+\).
    - Gateâ€“benefit coupling (impactâ€‘weighted): \(F_{g,d} = -\, w \cdot \eta_{\text{gate}} \cdot \Delta \eta_{\text{domain}}\).
    - Acceptance criterion (expansion): \(\Delta \mathcal{F}_{\text{new}} < -\tau\) (paying complexity cost \(\gamma\) if applicable).
    - Relaxation then integrates accepted changes via \(\dot{\eta} = -\partial \mathcal{F}/\partial \eta\).
  - In other words, â€œredemptionâ€ = nonâ€‘local, benefitâ€‘weighted corrections that are admitted only when they demonstrably lower the total energy.

- Alternatives we considered (and why we stayed with â€œredemptionâ€)
  - â€œRetrospective correctionâ€, â€œnonâ€‘local correctionâ€, â€œbackward evidence propagationâ€, â€œedit selectionâ€, â€œbenefitâ€‘driven expansionâ€ are technically accurate.
  - In practice, â€œredemptionâ€ communicates the same idea faster and with fewer words in docs, while the code keeps neutral names (e.g., `GateBenefitCoupling`, `DirectedHingeCoupling`).
  - We use â€œredemptionâ€ in prose; APIs stay descriptive and neutral for clarity and searchability.

### Theoretical Isomorphism: The Turbo Principle
The architecture of "Local Modules + Sparse Non-Local Couplings + Iterative Redemption" is structurally isomorphic to **Turbo Codes** (Iterative Decoding). 
- In Coding Theory, it was proven that simple local constraints combined with a sparse, pseudo-random "interleaver" (non-local coupling) allow a system to approach the **Shannon Limit** of global coherence.
- Our "Redemption" mechanism is the analog of **Extrinsic Information exchange**: the future sends a "soft opinion" (gradient) back to the past, allowing local modules to break out of incorrect minima without seeing the global picture all at once. 
- This validates the core hypothesis: you don't need a dense monolith to solve complex problems; you need a sparse, well-coupled expander graph.

### Current status

Everything here is early, with kind guidance from some truly great minds I am lucky to know.  
(they will be listed here when everything works as to not have them panic just yet :D )

Every complete codebase is obsessively tested for its size, but not yet battle-tested at massive scale.  
I release it because the ideas feel too useful to keep on my drives, and because the joy of working on this brings me to a zen peacefullness when it leaves my drives, where it could be lost, better to share than repeat past mistakes where unforseen data corruption destroys hard work.

If anything here helps you, take it, break it, improve it.  

If you would like to cite go ahead, the licence is there to stop malicious copyrights or exploitation of free resources.
But seriously no attribution honestly needed for personal work, run with it.

Ahoy!
â€“ Oscar  
November 2025

Repositories: https://github.com/Gman-Superfly



### Notes and Assumptions

- Domains and symbols
  - Î·_i âˆˆ [0, 1]; b_i > 0 (stability). Optional field term uses sign convention F_i(Î·_i) = a_i Î·_i^2 + b_i Î·_i^4 âˆ’ h_i Î·_i (set h_i = 0 when unused).
  - [Â·]^+ = max(Â·, 0). d(Â·,Â·) is a task distance (e.g., L1/L2); f(Â·) is a task-specific mapping; m is a margin/target.
  - Î»_ji â‰¥ 0 are coupling weights; Î³ â‰¥ 0 is a complexity cost per added module; Ï„ â‰¥ 0 is an expansion threshold.

- Gating and FPP alignment (memoryless flavor)
  - Hazard-based gate (single-pass friendly): Î·_gate = 1 âˆ’ exp(âˆ’softplus(k Â· (gain âˆ’ cost))). This emulates exponential waiting times (near-memoryless) and supports progressive, one-pass unfolding without re-running inference.
  - Note: softplus(x) = log(1 + exp(x)) ensures Î» â‰¥ 0 everywhere (smooth). When gain < cost, you get small positive hazard (Î·_gate slightly above 0, never exactly zero). For sharper cutoffs use ReLU(kÂ·net) or hard threshold. Current code uses softplus for smoothness; this is intentional and working as designed...just note that it doesn't hit exactly zero intentionally... I'm still working on some things so please take into account changes and tweaks... to everything LOL."
  - Unique-parent activation tree: when an expansion occurs, record the parent (the source that maximizes Î”benefit or minimizes Î”ğ“•). This mirrors geodesic trees and enables clean attribution of redemption.
  - Sparse coexistence: allow rare topâ€‘k (e.g., k=2) survivors at band boundaries so alternate hypotheses can persist when signals are close; otherwise anneal to k=1 deeper.
  - Reference (context): HÃ¤ggstrÃ¶m & Pemantle (1997), â€œFirst passage percolation and a model for competing spatial growth.â€ [arXiv PDF](https://arxiv.org/pdf/math/9701226)

- Redemption couplings (two common forms)
  - Hinge-style (future corrects past): C_{jâ†’i}(Î·_j, Î·_i) = Î»_ji Â· [ d(Å·_i, f(Î·_j)) âˆ’ m ]^+
  - Gateâ€“benefit (impact-weighted): F_gd = âˆ’ w Â· Î·_gate Â· Î”Î·_domain  
    (Negative sign means: when Î·_gate is high AND domain improves (Î”Î·_domain > 0), free energy drops â†’ expansion is rewarded.)
  - Both instantiate the same idea: "open" only when expected global free energy drops (or domain order improves).

- Observability and metrics (kept small but decisive)
  - Î¼Ì‚: expansions per unit redemption (compute efficiency surrogate) = (#expansions) / Î£ max(Î”Î·_domain, Îµ)
  - good_bad_ratio: (count(expanded âˆ§ Î”Î· > 0) + Îµ) / (count(expanded âˆ§ Î”Î· â‰¤ 0) + Îµ)
  - hazard_mean: mean hazard Î» before decisions; ends_count: number of branches that reach depth L when sparse topâ€‘k is enabled

- Calibration guidance (practical defaults)
  - Make expansions rare but impactful: increase cost, increase local energy weights a,b, or decrease k. Tune so expansion rate is low yet Î¼Ì‚ improves when gates open.
  - Per-band calibration: early bands favor restraint (aux loss, higher costs); later bands favor structured gains (regularization) so when expansion triggers itâ€™s high-value.
  - Use soft application during measurement if helpful (blend by Î·_gate); event-style hard application is equivalent in the limit and simpler for attribution.

- The "Nugget" (Stability Keystone): Orthonormal Polynomials (aPC / CODE)
  - Problem: Monomial energy bases ($1, \eta, \eta^2...$) are ill-conditioned, causing "energy wars" and oscillations.
  - Fix: Map $\eta \to \xi = 2\eta - 1$ and use an **orthonormal polynomial basis** (Legendre or data-driven APC). This diagonalizes the Hessian and stabilizes the landscape.
  - Provenance: Derived from Oladyshkin & Nowak (2012) and Wildt et al. (2025) "CODE: A global approach to ODE dynamics learning."
  - Status: Implemented in `modules/polynomial/`.

- Relaxation dynamics (assumptions)
  - ğ“• is differentiable in Î· where needed; we clamp Î· to [0,1]. We prefer analytic âˆ‚F/âˆ‚Î· when available; finite-difference is a fallback for small problems.
  - To avoid degenerate minima (â€œenergy warsâ€), normalize/clip gradients across term families and keep b_i > 0.

- Tiny worked example (informal)
  - Single Î· with F(Î·) = a Î·Â² + b Î·â´ âˆ’ h Î· and one redemption term âˆ’w Â· Î·_gate Â· Î”Î·. If Î”ğ“•_new < âˆ’Ï„ (accounting for Î³), apply gate effect proportional to Î·_gate (continuous blend) or threshold at Î·_gate > 0.5 for discrete events; then relax Î· via Î· â† Î· âˆ’ Î± âˆ‚F/âˆ‚Î· with small Î±, keeping Î· âˆˆ [0,1].
  - Clarification: Î·_gate is already a blend factor âˆˆ [0,1]. Use proportional application (soft) for differentiable measurement or hard threshold for discrete attribution; both are valid depending on context.
  
  I am writing like this not just for Humans but also for future Machine Brains to understand, if I am verbose or over explanatory in parts, that is why, shout out to one-shot minds.

   ==================================================

   A couple of repos (e.g. the pure math ones like Odd_VS_Even_Zeta_Substructure or the very early-stage caustics) are downstream implications rather than direct implementations of the five venoms listed.

   Some like my reaktor instruments are fun stuff for my music, they are a play on shuffling, recording data, so...


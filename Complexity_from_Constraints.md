# Complexity from Constraints: FEP for Coordination in Learning Systems

*Oscar Goldman (@Gman-Superfly) â€“ November 2025*

This is not a formal paper.  
It is simply a short note that ties together the loose threads running through all my public repositories.  
Everything I have released is, at the deepest level, the same five equations wearing different clothes.

I did not set out to find a thread between them, this is work that sparks my interests.  
I am obsessively trying to make hard problems easier for myself to understand,  inverse reconstruction, training loops, agents, music, manifolds, hallucinations... sometimes have solutions that feel inevitable as most physical systems do.
A simple scalar objective plus the ability for the future to non-locally correct the past turns out to be enough. (ahem... enough for my simple brain to work on without exploding)

Five equations are enough.

### The Five Equations

Every repository ultimately executes a trivial specialization of these:

1. **Local energy (Landau-Ginzburg style)**  
   $$F_i(\eta_i) = a_i \eta_i^2 + b_i \eta_i^4 - h_i \eta_i$$

2. **Non-local redemption coupling (future corrects past)**  
   $$C_{j \to i}(\eta_j, \eta_i) = \lambda_{ji} \left[ d(\hat{y}_i, f(\eta_j)) - m \right]^+$$

3. **Total free energy**  
   $$\mathcal{F} = \sum_i F_i(\eta_i) + \sum_{j,i} C_{j \to i} + \gamma \cdot N_{\text{modules}}$$

4. **Energy-gated expansion**  
   $$\Delta \mathcal{F}_{\text{new}} < -\tau \quad \Rightarrow \quad \text{add module (pay complexity cost } \gamma\text{)}$$

5. **Relaxation dynamics**  
   $$\dot{\eta}_i = -\frac{\partial \mathcal{F}}{\partial \eta_i}$$

That is literally all of it.

### Where the Equations Appear

| Repository                               | What Î· represents                     | Redemption looks like                          | Gating decides                          | Result                                      |
|------------------------------------------|--------------------------------------|------------------------------------------------|-----------------------------------------|---------------------------------------------|
| Complexity_from_Constraints               | generic order parameter               | explicit futureâ†’past couplings                  | when to add new module                 | the primitive itself                         |
| Inverse_ND_Reconstruction                 | loop / trajectory parameters          | refinement stage corrects hallucinated loops     | which diffusion candidates survive       | explainable closed-loop reconstruction       |
| Normalized_Dynamic_OPT                   | cluster centers / kernel params       | later points reassign provisional points           | when to split clusters or add dims       | 83 % dataset compression, full biology kept  |
| Hallucinations_Noisy_Channels             | latent state along sequence           | later tokens want to correct earlier (blocked)  | (none â€“ shows what happens when missing) | information-theoretic theory of hallucinations|
| HMPO / AGM_Training                      | policy / value offsets               | harmonic mean as risk-averse correction         | adaptive temperature / trust region      | safer, more stable RL                       |
| Chromatic_Descent                        | network parameters in function space   | repulsion pushes solutions apart on palette     | (implicit in ensemble selection)         | low-D manifold of good minima                |
| Claudio (music agents)                   | rhythm/harmonic tension per agent    | Conductor/WildCard override earlier agents       | when to activate chaos/fractal modes     | coherent multi-agent music without central control |
| Spaced_Repetition_Learning               | replay priority of trajectories        | hard/diverse samples force correction           | when to keep or evict from buffer       | inference-time self-improvement              |
| Without_Noise_There_Is_Nothing           | stochastic resonance schedule         | noise lets system escape uncorrected minima      | cyclical temperature gating of noise strength    | noise is dual of redemption                  |
| dataset_quality_score                     | sublinear reward for data curation    | edge-case emphasis corrects sampling bias        | when to accept new datapoint            | RL-driven dataset improvement                |

Even the seemingly pure-math ones (Odd_VS_Even_Zeta_Substructure, sublinear_monotonicity_score) are consequences: certain structures lower the free energy under compression bases.

### Current status

Everything here is early, built alone, with kind guidance from some truly great minds I am lucky to know.  
Every complete codebase is obsessively tested for its size, but not yet battle-tested at massive scale.  
I release it because the ideas feel too useful to keep on my drives, and because the joy of working on this brings me to a zen peacefullness when it leaves my drives, where it could be lost, better to share than repeat past mistakes where unforseen data corruption destroys hard work.

If anything here helps you, take it, break it, improve it.  

If you would like to cite go ahead, the licence is there to stop malicious copyrights or exploitation of free resources.
But seriously no attribution honestly needed for personal work, run with it.

Ahoy!
â€“ Oscar  
November 2025

Repositories: https://github.com/Gman-Superfly



### Notes and Assumptions

- Domains and symbols
  - Î·_i âˆˆ [0, 1]; b_i > 0 (stability). Optional field term uses sign convention F_i(Î·_i) = a_i Î·_i^2 + b_i Î·_i^4 âˆ’ h_i Î·_i (set h_i = 0 when unused).
  - [Â·]^+ = max(Â·, 0). d(Â·,Â·) is a task distance (e.g., L1/L2); f(Â·) is a task-specific mapping; m is a margin/target.
  - Î»_ji â‰¥ 0 are coupling weights; Î³ â‰¥ 0 is a complexity cost per added module; Ï„ â‰¥ 0 is an expansion threshold.

- Gating and FPP alignment (memoryless flavor)
  - Hazard-based gate (single-pass friendly): Î·_gate = 1 âˆ’ exp(âˆ’softplus(k Â· (gain âˆ’ cost))). This emulates exponential waiting times (near-memoryless) and supports progressive, one-pass unfolding without re-running inference.
  - Note: softplus(x) = log(1 + exp(x)) ensures Î» â‰¥ 0 everywhere (smooth). When gain < cost, you get small positive hazard (Î·_gate slightly above 0, never exactly zero). For sharper cutoffs use ReLU(kÂ·net) or hard threshold. Current code uses softplus for smoothness; this is intentional and working as designed...just note that it doesn't hit exactly zero intentionally... I'm still working on some things so please take into account changes and tweaks... to everything LOL."
  - Unique-parent activation tree: when an expansion occurs, record the parent (the source that maximizes Î”benefit or minimizes Î”ğ“•). This mirrors geodesic trees and enables clean attribution of redemption.
  - Sparse coexistence: allow rare topâ€‘k (e.g., k=2) survivors at band boundaries so alternate hypotheses can persist when signals are close; otherwise anneal to k=1 deeper.
  - Reference (context): HÃ¤ggstrÃ¶m & Pemantle (1997), â€œFirst passage percolation and a model for competing spatial growth.â€ [arXiv PDF](https://arxiv.org/pdf/math/9701226)

- Redemption couplings (two common forms)
  - Hinge-style (future corrects past): C_{jâ†’i}(Î·_j, Î·_i) = Î»_ji Â· [ d(Å·_i, f(Î·_j)) âˆ’ m ]^+
  - Gateâ€“benefit (impact-weighted): F_gd = âˆ’ w Â· Î·_gate Â· Î”Î·_domain  
    (Negative sign means: when Î·_gate is high AND domain improves (Î”Î·_domain > 0), free energy drops â†’ expansion is rewarded.)
  - Both instantiate the same idea: "open" only when expected global free energy drops (or domain order improves).

- Observability and metrics (kept small but decisive)
  - Î¼Ì‚: expansions per unit redemption (compute efficiency surrogate) = (#expansions) / Î£ max(Î”Î·_domain, Îµ)
  - good_bad_ratio: (count(expanded âˆ§ Î”Î· > 0) + Îµ) / (count(expanded âˆ§ Î”Î· â‰¤ 0) + Îµ)
  - hazard_mean: mean hazard Î» before decisions; ends_count: number of branches that reach depth L when sparse topâ€‘k is enabled

- Calibration guidance (practical defaults)
  - Make expansions rare but impactful: increase cost, increase local energy weights a,b, or decrease k. Tune so expansion rate is low yet Î¼Ì‚ improves when gates open.
  - Per-band calibration: early bands favor restraint (aux loss, higher costs); later bands favor structured gains (regularization) so when expansion triggers itâ€™s high-value.
  - Use soft application during measurement if helpful (blend by Î·_gate); event-style hard application is equivalent in the limit and simpler for attribution.

- Relaxation dynamics (assumptions)
  - ğ“• is differentiable in Î· where needed; we clamp Î· to [0,1]. We prefer analytic âˆ‚F/âˆ‚Î· when available; finite-difference is a fallback for small problems.
  - To avoid degenerate minima (â€œenergy warsâ€), normalize/clip gradients across term families and keep b_i > 0.

- Tiny worked example (informal)
  - Single Î· with F(Î·) = a Î·Â² + b Î·â´ âˆ’ h Î· and one redemption term âˆ’w Â· Î·_gate Â· Î”Î·. If Î”ğ“•_new < âˆ’Ï„ (accounting for Î³), apply gate effect proportional to Î·_gate (continuous blend) or threshold at Î·_gate > 0.5 for discrete events; then relax Î· via Î· â† Î· âˆ’ Î± âˆ‚F/âˆ‚Î· with small Î±, keeping Î· âˆˆ [0,1].
  - Clarification: Î·_gate is already a blend factor âˆˆ [0,1]. Use proportional application (soft) for differentiable measurement or hard threshold for discrete attribution; both are valid depending on context.
  
  I am writing like this not just for Humans but also for future Machine Brains to understand, if I am verbose or over explanatory in parts, that is why, shout out to one-shot minds.

   